% This file was adapted from ICLR2022_conference.tex example provided for the ICLR conference
\documentclass{article} % For LaTeX2e
\usepackage{collas2025_conference,times}
\usepackage{easyReview}
\usepackage{booktabs, multicol, multirow}
\usepackage{colortbl}
\usepackage{tikz}
\usepackage{tikz-3dplot}
\usetikzlibrary{arrows.meta} 
\usetikzlibrary{calc}
\usetikzlibrary{positioning}

\usepackage{amsthm}
\usepackage{amsmath}

\DeclareMathOperator*{\concat}{%
    \mathchoice%
        {\Big\Vert}%
        {\big\Vert}%
        {\Vert}%
        {\Vert}%
}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% Please leave these options as they are
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    filecolor=magenta,
    urlcolor=blue,
    citecolor=purple,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }




\title{Efficient Online Trajectory User Linking with Multi-Level Spatial Embedding Sharing}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \collasfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro  \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Random University\\
Country \\
\texttt{\{hippo,brain\}@cs.random.edu} \\
\And % Use And to have authors side by side
Koala Learnus \& D. Q. ResNet  \\
Department of Computational Neuroscience \\
University of Random City \\
Another Country \\
\texttt{\{koala,net\}@random.rand} \\
\AND % Use AND to have authors block one under the other
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\collasfinalcopy % Uncomment for camera-ready version, but NOT for submission.

%\preprintcopy % Uncomment for the preprint version, but NOT for submission.

\begin{document}


\maketitle

\begin{abstract}
    Trajectory-User Linking (TUL), which aims to associate unlabeled spatial trajectories to a user or entity that generated them is a common task in transportation and mobility applications.
    Often times, data in such applications is generated as part of a data stream and may be affected by distributional shifts.
    For this reason, training TUL models \textit{online} by incrementally adapting them to new data can be beneficial in many cases.
    Nevertheless, online TUL has not been studied by existing research.
    To bridge this gap, we perform comprehensive online learning evaluations of some the most successful TUL techniques.
    We further propose a novel embedding approach for online TUL, which we call \underline{M}ult\underline{i}-\underline{L}evel \underline{E}mbedding \underline{S}haring (MiLES).
    MiLES involves partially sharing embeddings for locations within neighborhoods of multiple size levels, allowing the shared embeddings to receive more frequent updates and therefore enabling faster adaptation to new trajectory data.
    Our evaluations on multiple real-world datasets show that MiLES significantly improves the performance of existing state-of-the-art TUL approaches in an online learning setting.
\end{abstract}

\section{Introduction}\label{sec:intro}

Thanks to the proliferation of smartphones, wearables and other devices supporting location services, mobility data has become abundant, spawning a variety of machine learning tasks in the process \citep{rehmanMiningPersonalData2015,zhengTrajectoryDataMining2015}.
One such task is Trajectory User Linking (TUL), introduced by \citet{gaoIdentifyingHumanMobility2017}.
- Trajectory: temporally ordered sequence of so-called check-ins at visited locations or points of interest (POIs)
- TUL involves linking trajectories, to the users or entities that created them \citep{gaoIdentifyingHumanMobility2017}
- TUL has practical applications in disease control, law enforcement, ride-sharing, location recommendation and many more \citep{haoUnderstandingUrbanPandemic2020,gaoIdentifyingHumanMobility2017}
- Previous works on TUL achieved remarkable results using approaches based on recurrent neural networks and, more recently, on transformer architectures.
- These works are limited to conventional batch-learning where all training data is available at once.
- However, when establishing a new location-based service for instance, the amount of initial data available for training may be insufficient for achieving optimal results with this approach, due to the fact that in many real-world applications, data is instead generated one sample or chunk at a time in the form of a \textit{data stream}.
- Furthermore, such data streams are commonly affected by temporal dependencies and shifts in their distributions in the form of concept drift \citep{gomesSurveyEnsembleLearning2017}.
- For mobility data, distributional shifts notably occurred as a result of the COVID-19 pandemic \citep{borkowskiLockdownedEverydayMobility2021} but may, for example, also occur due to seasonality, new points of interest, or changes in user activity.
- Therefore, incremental adaptation to new trajectories through the use of \textit{online learning} can be essential in real-world applications of TUL.

According to \citet{bifetMOAMassiveOnline2010}, an online machine learning model operating in a streaming environment must be able to
% Remove this? Or does it help understand what online learning means?
\begin{enumerate}
    \item[R1:] process a single instance at a time,\label{rq:single_instance}
    \item[R2:] process each instance in a limited amount of time,\label{rq:limited_time}
    \item[R3:] use a limited amount of memory,\label{rq:limited_memory}
    \item[R4:] predict at any time,\label{rq:predict_any_time}
    \item[R5:] adapt to changes in the data distribution.\label{rq:adapt_to_drift}
\end{enumerate}

% While most existing TUL approaches can easily be adapted to fulfill these requirements by executing a training step every time a new trajectory is received, the requirements to predict at any time (R4) and adapt to changes in the data distribution (R5)
- In theory, most existing TUL approaches can easily be adapted to fulfill these requirements by executing a training step every time a new trajectory is received
- Approach for embedding check-ins of existing TUL approaches may negatively impact their ability to predict at any time (R4) and adapt to changes in the data distribution (R5)
- Most models embed check-ins using a lookup table containing embedding vectors for each individual POI
- Due to large number of different locations, visits at most individual POIs is generally infrequent \citep{chenMutualDistillationLearning2022a}
- Only embeddings of visited POIs have non-zero gradient
- Not a big issue in batch-learning, due to training on dataset collected over a long timespan
- Causes embeddings to adapt slowly in online TUL
- Sharing embeddings between multiple locations causes them to have significantly lower gradient sparsity
- Intuitively knowledge about locations is shared enabling the network to interpret ones with that received few check-ins
- Sharing embeddings based on spatial proximity/neighborhoods *makes sense*
- POI's withing the same neighborhood often share similar characteristics, like function (i.e. working, living, recreation) or affluence
- Using proximity as an inductive bias also requires no additional information
- Embedding gradients get less sparse with increasing neighborhood size
- However, there is a trade-off between density and informational content as embeddings also become less specific with increasing neighborhood-size
- Therefore, sharing between large neighborhoods may be helpful at the start of training and after concept drifts but inhibits the models capability to accurately interpret precise locations
- For this reason, we propose multi-level embedding sharing (MiLES), which pools different parts of the embeddings at different levels of neighborhood sizes
- MiLES divides embeddings into multiple segments, where each segment is shared between neighborhoods of varying sizes to enable a spectrum of features with varying informational content and adaptation speed
- Different segments may be prioritized based on whether density/adaptation speed or feature informational content is more important at the respective stage of the online learning process

- To analyse the effectiveness of MiLES, we first perform a broad evaluation of existing (state-of-the-art) TUL techniques in an online learning setting (i)
- We then repeat the same evaluations with each of the techniques adapted to use our proposed MiLES embedding technique finding significant performance improvements (ii)
- Lastly, we perform an ablation study for each component of MiLES and conduct an in-depth analysis of its functionality (ii)


\section{Preliminaries}\label{sec:preliminaries}

In the following we will introduce the basic concepts underlying online trajectory user linking:

% As previously noted, the fundamental units of information for TUL are so-called check-ins which are defined as follows:

\begin{definition}[Check-In]
    A check-in is a tuple $\vc = (u, t, \vl)$ containing a user identifier $u$ from a set of participating users $\sU$, timestamps $t$ and geo-coordinates $\vl$ of the visited location.
    Check-ins often additionally contain a unique identifier $p\in\sP$, where $\sP$ is a finite set of POI identifiers.
\end{definition}

\begin{definition}[Trajectory]
    A trajectory is a chronologically ordered sequence of $n$ check-ins $[(u, t_0, \vl_0), ..., (u, t_n, \vl_n)]$ generated by a user $u$ within a specific timeframe $\tau$. If the check-ins of a trajectory lack a user label $u$, they are called \textit{unlinked}.
\end{definition}

\begin{definition}[Trajectory User Linking] %TODO: convert to problem instead of definition
    The task of trajectory user linking is to learn a function $f$ on a training set of historical trajectories $\sT_{(\rm{train})}$, that links a set of \textit{unlinked} trajectories $\sT^{(\rm{test})} = \{T_0, ..., T_m\}$ to the users $\sU^{(\rm{test})} = \{u_0,...,u_m\}$ that generated them. Formally, the objective of TUL can be described as
    \begin{equation}
        \min_{\bm{\theta}} \sum_{i=0}^{m} L(f(T_i; \bm{\theta}, \sT^{(\rm{train})}), u_i), \ T_i \in \sT^{(\rm{test})}, u_i \in \sU^{(\rm{test})},
    \end{equation}
    where $L$ is a loss function that quantifies the predictive error and $\bm{\theta}$ are the model parameters to be optimized.
\end{definition}

% TODO: move this to approach?
Due to requirements \textbf{R1-R5} the above definition of TUL, where the model is trained and tested on seperate, self-contained is impractical for an online learning setting.
Instead, online TUL can more accurately be analyzed using the \textit{prequential} or \textit{interleaved test-then-train} evaluation scheme \citep{bifetMOAMassiveOnline2010}, where each new sample is first used to validate and then to train the underlying model.
According to this scheme, the optimization goal of online TUL can be formulated as
\begin{equation} % Notation kinda ugly 
    \min_{\bm{\theta}_0, ..., \bm{\theta}_{m-1}} \sum_{i=1}^{m-1} L(f(T_{i}; \bm{\theta}_{i-1}, \sT^{(\rm{stream})}_{:i-1}), u_{i}), \ T_i \in \sT^{(\rm{stream})}, u_i \in \sU^{(\rm{stream})},
\end{equation}
where $\bm{\theta}_{i-1}$ denotes the model parameters at timestep $i-1$ and $\sT^{(\rm{stream})}_{:i-1}$ is the set of all samples in the data stream up until timestep $i-1$.
A key difference between the batch-mode and the online learning formulation of TUL is that for the latter, the parameters at each step of the training process contribute equally to the performance of the model, whereas for the former, only the parameters achieved at the very end of the training process are relevant.
For this reason, the sparsity issue of existing location embedding techniques mentioned in \ref{sec:intro} can have a negative impact in online TUL.


\section{Related Work}

While trajectory user linking itself is a relatively recent task, several older approaches from adjacent fields are also suitable for it.
For instance, using the longest common subsequence algorithm \citep{yingSemanticTrajectoryMining2011} one can predict the user label of an unlinked trajectory based on the trajectory sharing the longest sub-trajectory with the input.

By encoding trajectories solely based on the number of occurrences of each POI, using a bag-of-words approach \citep{mikolovEfficientEstimationWord2013}, conventional classification techniques such as linear discriminant analysis or support vector machines can be used for TUL.

More recent approaches starting with TUL via Embedding and RNN (TULER) \citep{gaoIdentifyingHumanMobility2017}, instead employ an embedding scheme that maintains the order of check-ins by replacing each location in a trajectory with its corresponding real-valued vector stored in an embedding table $Z \in \sR^{|\sP| \times D}$, where $|P|$ is the number of unique locations and $D$ the embedding dimension.
\citet{gaoIdentifyingHumanMobility2017} introduced three different TULER variants, TULER-G, TULER-L and BiTULER, combining this embedding approach with either a GRU- \citep{choPropertiesNeuralMachine2014a}, an LSTM- or a bidirectional LSTM network \citep{hochreiterLongShort1997}.
In subsequent works, various extensions of TULER were proposed.
TULVAE \citep{zhouTrajectoryUserLinkingVariational2018} for instance, combines an LSTM classifier with a variational autoencoder \citep{kingmaAutoEncodingVariationalBayes2013a}, which receives classifications as additional input.
With this approach, the LSTM classifier is additionally trained to aid the reconstruction of trajectories.
DeepTUL proposed by \citet{miaoTrajectoryUserLinkingAttentive2020} instead adds a historical attention module that generates a context vector based on the user IDs of all previous check-ins that share the same locations and time-slots as the input trajectory.
This context vector is then provided as an additional input to an RNN classifier.

With their advancement in other machine learning disciplines, newer studies on TUL have increasingly focused on transformer-based approaches.
The T3S model \citep{yangT3SEffectiveRepresentation2021}, for example, combines a transformer- and an LSTM-encoder for encoding trajectories before feeding them to a classification layer.
Unlike most other TUL approaches, T3S embeds check-ins by mapping them to cells of a square grid and using the respective grid cells as the keys for a lookup table.
Consequently, locations within the same grid cell share their embeddings and no POI information is removed.
uses a transformer that encodes the sequence of visited grid cells which is concatenated with an LSTM embedding of the coordinate sequence

In a similar fashion, the purely transformer-based TULHOR \citep{alsaeedTrajectoryUserLinkingUsing2023a} embeds check-ins based on their position within a hexagonal grid.
The algorithm supplements the grid-based embeddings with conventional POI embeddings as well as with the embeddings of the grid cells visited along the fastest routes between individual check-in locations.
Seeking to combine the benefits of transformers and RNNs, \citet{chenMutualDistillationLearning2022a} proposed MainTUL, which simultaneously trains one of both types of models on trajectories augmented with previous trajectories of the same user.
To enhance the models performance, the outputs of the transformer are distilled into the RNN.

Further TUL models include GNNTUL \citep{zhouTrajectoryUserLinkingGraph2021a} and AttnTUL \citep{chenTrajectoryUserLinkingHierarchical2024}, which process trajectories using graph neural networks, as well as the Siamese neural network TULSN \citep{yuTULSNSiameseNetwork2020a}.



\section{Approach}

As previously described, sharing embeddings between a greater number of locations increases their gradient density but reduces their informational content.
% Add proof for this? Or is this trivial? 
Embedding techniques of existing TUL approaches generally prioritize informational content by, if at all, sharing embeddings within small neighborhoods, due to the fact that the negative effect of gradient sparsity can be mitigated by training on a large amount of data.

However, this is not possible in the online learning setting.
Instead, due to the necessity to quickly adapt to new data, both density and informational content may vary in terms of impact on the model's performance throughout a data stream.
By proposing multi-level embedding sharing (MiLES) we therefore seek to provide an alternative solution by generating embedding features that cover a wide area of the density-information spectrum.
In the following, we will give an in-depth description of the functionality of MiLES, which is also depicted in Figure~\ref{fig:approach}.

% As described in Section~\ref{sec:intro}, the POI-based embedding approach, of most existing TUL models may limit their ability to quickly adapt to a data stream.
% To mitigate this issue while maintaining the model's ability to distinguish between precise locations, we propose the multi-level embedding sharing approach (MiLES).

Like TULHOR's embedding approach \citep{alsaeedTrajectoryUserLinkingUsing2023a}, MiLES maps locations to a grid created as a tiling of regular hexagons, which we selected due to their better representation of euclidean distance based neighborhoods.
However, unlike in TULHOR, we repeat this process multiple times with increasing cell sizes and therefore increasing levels of aggregation.

\begin{figure}[ht]
    \centering
    \resizebox{\textwidth}{!}{
        \input{./figs/approach.tex}
    }
    \caption{Visualization of the proposed multi-level embedding sharing (MiLES) technique.}
    \label{fig:approach}
\end{figure}

Accordingly, we augment check-ins with an additional index $h_l$ for each embedding-level $l \in \{0, 1, ..., l^{(\rm{max})}\}$, which identifies the grid cell containing the check-in location.
If available, we leverage POI information by setting $h_0$ to the POI index.
We further initialize an embedding matrix $\mZ_{l}$ of shape $h_l^{\rm{(max)}} \times d_l$ for each level, where $d_l$ is the embedding dimension and $h_l^{\rm{(max)}}$ the maximum POI- or grid cell index.
To account for the decreasing informational content with increasing levels of aggregation, we assign smaller dimensions to higher-level embeddings.
We compute the individual embedding dimensions as
\begin{equation}
    d_l=\frac{d \cdot \alpha^{-l}}{\sum_{l=0}^{l^{\rm{(max)}}} \alpha^{-l}}, \ \alpha > 1
\end{equation}
where $d$ is the dimension of the final embedding and $\alpha$ is a hyperparameter.
By default we use $\alpha=2$ in our experiments.

To compute the final embedding, we concatenate all level-specific embeddings, each weighted by a learnable parameter $w_l$.
Using $\concat$ to represent vector concatenation, the embedding function $g$ can be defined as
\begin{equation}
    g(\vh; \mZ_0, \mZ_1, ..., \mZ_{l^{\rm{(max)}}} \vw) = \concat_{l=0}^{l^{\rm{(max)}}} \mZ_{l,h_l,:} \cdot w_l,
\end{equation}
where $\mZ_{l,h_l,:}$ is the row vector of $\mZ_l$ located at index $h_l$.

We select concatenation instead of summation to aggregate the level-specific embeddings to avoid interference between levels.
For a fixed total embedding dimension, this approach also significantly reduces the number of parameters compared to a purely POI-based or summed multi-level embedding, since sharing embeddings between multiple locations decreases the size of the higher level embedding matrices.

By multiplying the level-specific embeddings $\mZ_{l,h_l,:}$ with learnable parameters $w_l$, we allow the average embedding-norm to be optimized on a per-level basis throughout the online learning process.
This can be interpreted as an attention mechanism that adapts the model's attention to individual embedding features based on previously observed data.
With this mechanism, the model can prioritize different embedding features based on the importance of gradient density or informational content for the current state of the data stream.

% Remove this?
Another advantage of MiLES is that due to only adding an $O(1)$ table lookup and a $O(d_l)$ vector multiplication for each embedding level, its computational overhead is minimal.

\section{Experiments}

To evaluate the impact of the proposed MiLES approach and its individual components on the performance of existing TUL models in a data stream setting, we perform a multitude of experiments.

We conduct our experiments using the popular public trajectory datasets GeoLife, Foursquare-NYC and Foursquare-TKY, which we pre-process according to \citet{chenMutualDistillationLearning2022a} by splitting each trajectory into shorter trajectories with a maximum length of 24 hours for Foursquare-NYC and Foursquare-TKY and 3 hours for GeoLife and select the most active users.
For more information on the selected datasets, refer to Table~\ref{tab:datasets}.

To adhere to the test-then-train evaluation scheme described in Section~\ref{sec:preliminaries}, we process the datasets one trajectory at a time and use each of them to first evaluate and then to test the TUL model.
In terms of data modalities, we use GPS coordinates, timestamps and, for the Foursquare datasets, which include such information, POI identifiers.
We further provide hour-specific lookup embeddings as additional model inputs, as was commonly done in previous work \citep[see e.g.][]{chenMutualDistillationLearning2022a,miaoTrajectoryUserLinkingAttentive2020}.
For better comparability, we omit any model components that require additional information like the mobility flows used by TULHOR.
For the evaluation of MainTUL and DeepTUL, which require historical data as inputs, we maintain a buffer of the most recent 1000 trajectories during each run.

For approach-specific hyperparameters, we use the authors suggested default values and tune other hyperparameters like the learning rate, number of layers and model size on the first 5000 trajectories of Foursquare-TKY.
We use the same approach to tune the hyperparameters of the proposed embedding technique, which we implement by dividing the map-area containing all check-in locations based on multiple hexagonal grids, with varying numbers of rows.
By default, we use a total of 3 embedding sharing levels with 200 grid rows for the base level and halve the number of rows for each subsequent embedding level.

Unless stated otherwise, we repeat each of our experiments with 5 different random seeds and report the averages over all runs.

\begin{table}[h]
    \centering
    \caption{Datasets used for experimental evaluation.}
    \label{tab:datasets}
    \begin{tabular}{@{}lrrrrr@{}}
        \toprule
        Dataset        & \# Users & \# Trajectories & \# Check-Ins & \# POIs & Timespan  \\ \midrule
        Foursquare-NYC & 800      & 61,218          & 196,435      & 34,383  & 10 months \\
        Foursquare-TKY & 800      & 70,007          & 324,564      & 38,212  & 10 months \\
        GeoLife        & 150      & 25,611          & 1,284,208    & 0       & 64 months \\ \bottomrule
    \end{tabular}
\end{table}

In the following, we will describe the the results of our prequential evaluations.

To evaluate their suitability for online learning applications, we ran experiments with a variety of existing TUL approaches and their original location embedding techniques.
The results of these experiments, depicted in Table~\ref{tab:models}, show that the bidirectional LSTM-based BiTULER achieved the overall best performance, with the only exception being the GeoLife dataset where DeepTUL yielded a higher top-1 accuracy and macro f1 score.
A likely reason for this is that as the simplest model with the lowest number of parameters allowing for faster adaptation to the distribution of the data stream.
The same effect may also explain the overall lower performance of the transformer-based MainTUL, T3S and TULHOR models compared to the RNN-based alternatives.

\begin{table}[h]
    \caption{Top-1 accuracy (Acc@1), top-5 accuracy (Acc@5) and macro F1 score [\%] of TUL models averaged over prequential evaluation runs on different datasets.}
    \label{tab:models}
    \begin{tabular}{lccccccccc}
        \toprule
        Dataset  & \multicolumn{3}{c}{Foursquare-NYC} & \multicolumn{3}{c}{Foursquare-TKY} & \multicolumn{3}{c}{GeoLife}                                                                                                             \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
        Model    & Acc@1                              & Acc@5                              & Macro F1                    & Acc@1           & Acc@5           & Macro F1        & Acc@1           & Acc@5           & Macro F1        \\
        \midrule
        Bi-TULER & \bfseries 60.12                    & \bfseries 67.20                    & \bfseries 57.83             & \bfseries 61.28 & \bfseries 73.08 & \bfseries 58.96 & \bfseries 37.56 & 70.85           & 26.69           \\
        TULVAE   & 59.79                              & 66.77                              & 57.32                       & 54.19           & 64.92           & 49.85           & 37.08           & 70.45           & 25.25           \\
        DeepTUL  & 58.72                              & 65.48                              & 56.60                       & 59.14           & 70.66           & 56.90           & 36.32           & \bfseries 72.64 & \bfseries 29.82 \\
        MainTUL  & 55.67                              & 62.61                              & 53.01                       & 56.81           & 69.18           & 54.09           & 34.00           & 70.26           & 21.76           \\
        T3S      & 52.98                              & 60.28                              & 49.50                       & 53.65           & 66.30           & 50.38           & 35.25           & 71.11           & 21.52           \\
        TULHOR   & 53.85                              & 61.13                              & 50.40                       & 54.39           & 67.24           & 51.06           & 34.65           & 72.46           & 24.92           \\
        \bottomrule
    \end{tabular}
\end{table}

We subsequently repeated our experiments with different TUL approaches, replacing their embedding modules with the propose MiLES technique. 
Table~\ref{tab:miles_improvements} shows the relative change in performance metrics measured in percentage points achieved with MiLES compared to each models original embedding technique. 


\begin{table}[h]
    \caption{Change of top-1 accuracy (Acc@1), top-5 accuracy (Acc@5) and macro F1 Score metrics in percentage points when using MiLES over each models' original embedding technique. Larger gains are shown in a darker hue.}
    \label{tab:miles_improvements}
    \begin{tabular}{lccccccccc}
        \toprule
        Dataset  & \multicolumn{3}{c}{Foursquare-NYC}                    & \multicolumn{3}{c}{Foursquare-TKY}                    & \multicolumn{3}{c}{GeoLife}                                                                                                                                                                                                                                                                                                                                                                           \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
        Model    & Acc@1                                                 & Acc@5                                                 & Macro F1                                              & Acc@1                                                 & Acc@5                                                 & Macro F1                                              & Acc@1                                                 & Acc@5                                                 & Macro F1                                              \\
        \midrule
        Bi-TULER & {\cellcolor[HTML]{E8F59F}} \color[HTML]{000000} +1.49 & {\cellcolor[HTML]{BFE47A}} \color[HTML]{000000} +3.58 & {\cellcolor[HTML]{E3F399}} \color[HTML]{000000} +1.71 & {\cellcolor[HTML]{E8F59F}} \color[HTML]{000000} +1.44 & {\cellcolor[HTML]{CFEB85}} \color[HTML]{000000} +2.87 & {\cellcolor[HTML]{E8F59F}} \color[HTML]{000000} +1.44 & {\cellcolor[HTML]{36A657}} \color[HTML]{000000} +8.72 & {\cellcolor[HTML]{6BBF64}} \color[HTML]{000000} +6.98 & {\cellcolor[HTML]{7FC866}} \color[HTML]{000000} +6.26 \\
        TULVAE   & {\cellcolor[HTML]{E2F397}} \color[HTML]{000000} +1.79 & {\cellcolor[HTML]{BDE379}} \color[HTML]{000000} +3.73 & {\cellcolor[HTML]{DFF293}} \color[HTML]{000000} +2.04 & {\cellcolor[HTML]{CFEB85}} \color[HTML]{000000} +2.90 & {\cellcolor[HTML]{B1DE71}} \color[HTML]{000000} +4.29 & {\cellcolor[HTML]{C7E77F}} \color[HTML]{000000} +3.22 & {\cellcolor[HTML]{4BB05C}} \color[HTML]{000000} +8.01 & {\cellcolor[HTML]{75C465}} \color[HTML]{000000} +6.57 & {\cellcolor[HTML]{A7D96B}} \color[HTML]{000000} +4.74 \\
        DeepTUL  & {\cellcolor[HTML]{EEF8A8}} \color[HTML]{000000} +1.06 & {\cellcolor[HTML]{CBE982}} \color[HTML]{000000} +3.04 & {\cellcolor[HTML]{EBF7A3}} \color[HTML]{000000} +1.26 & {\cellcolor[HTML]{EFF8AA}} \color[HTML]{000000} +0.94 & {\cellcolor[HTML]{D9EF8B}} \color[HTML]{000000} +2.44 & {\cellcolor[HTML]{EFF8AA}} \color[HTML]{000000} +0.95 & {\cellcolor[HTML]{39A758}} \color[HTML]{000000} +8.59 & {\cellcolor[HTML]{7DC765}} \color[HTML]{000000} +6.31 & {\cellcolor[HTML]{87CB67}} \color[HTML]{000000} +5.97 \\
        MainTUL  & {\cellcolor[HTML]{E8F59F}} \color[HTML]{000000} +1.44 & {\cellcolor[HTML]{AFDD70}} \color[HTML]{000000} +4.33 & {\cellcolor[HTML]{E6F59D}} \color[HTML]{000000} +1.52 & {\cellcolor[HTML]{E5F49B}} \color[HTML]{000000} +1.64 & {\cellcolor[HTML]{C1E57B}} \color[HTML]{000000} +3.49 & {\cellcolor[HTML]{E6F59D}} \color[HTML]{000000} +1.56 & {\cellcolor[HTML]{45AD5B}} \color[HTML]{000000} +8.19 & {\cellcolor[HTML]{82C966}} \color[HTML]{000000} +6.18 & {\cellcolor[HTML]{93D168}} \color[HTML]{000000} +5.50 \\
        T3S      & {\cellcolor[HTML]{E3F399}} \color[HTML]{000000} +1.71 & {\cellcolor[HTML]{C9E881}} \color[HTML]{000000} +3.13 & {\cellcolor[HTML]{DDF191}} \color[HTML]{000000} +2.10 & {\cellcolor[HTML]{E3F399}} \color[HTML]{000000} +1.71 & {\cellcolor[HTML]{CFEB85}} \color[HTML]{000000} +2.84 & {\cellcolor[HTML]{E0F295}} \color[HTML]{000000} +1.90 & {\cellcolor[HTML]{6BBF64}} \color[HTML]{000000} +6.98 & {\cellcolor[HTML]{A5D86A}} \color[HTML]{000000} +4.80 & {\cellcolor[HTML]{9BD469}} \color[HTML]{000000} +5.18 \\
        TULHOR   & {\cellcolor[HTML]{E3F399}} \color[HTML]{000000} +1.71 & {\cellcolor[HTML]{C9E881}} \color[HTML]{000000} +3.16 & {\cellcolor[HTML]{DDF191}} \color[HTML]{000000} +2.11 & {\cellcolor[HTML]{E2F397}} \color[HTML]{000000} +1.86 & {\cellcolor[HTML]{D1EC86}} \color[HTML]{000000} +2.75 & {\cellcolor[HTML]{DDF191}} \color[HTML]{000000} +2.10 & {\cellcolor[HTML]{45AD5B}} \color[HTML]{000000} +8.20 & {\cellcolor[HTML]{A0D669}} \color[HTML]{000000} +5.03 & {\cellcolor[HTML]{8ECF67}} \color[HTML]{000000} +5.68 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{figs/ablation_all.pdf}
    \caption{Results of ablation study.}
    \label{fig:ablation}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figs/discretization_rows_BiTULER.pdf}
    \caption{Performance on Foursquare-NYC relative to number of embedding levels and rows in the base level grid on the performance achieved with MiLES.}
    \label{fig:discretization}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics{figs/emb_prio.pdf}
    \caption{Mean weighting factors $w_0, ..., w_3$ and number of unique users within the last 1000 trajectories for evaluation runs on Foursquare-NYC. Shaded areas represent the 1$\sigma$ range.}
    \label{fig:emb_prio}
\end{figure}


\section{Conclusion}

- proposed novel embedding approach for online TUL
- evaluated efficacy of approach in combination with a variety of TUL models on three real-world datasets
- found improvements for all combinations of models and datasets
- improvements in terms of top 5 accuracy were greatest
- challenging GeoLife dataset benefitted the most
- embedding technique can be used for any task that takes localization data as input
- in terms of models, RNN-based ones generally outperformed transformer based approaches
- likely due to their faster convergence thanks to fewer parameters

\bibliography{collas2025_conference}
\bibliographystyle{collas2025_conference}

\appendix
\section{Appendix}
You may include other additional sections here.

\end{document}
