{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.data.hex_utils import * \n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/processed/geolife_hex_100.pkl\", 'rb') as f: \n",
    "    hdf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [f\"cell{i}\" for i in range(4)] + [\"user\"]:\n",
    "    hdf[col] += 1\n",
    "\n",
    "hdf.to_pickle(\"../data/processed/geolife_hex_100.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence, unpack_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(seq, lens):\n",
    "    t = pad_sequence(seq, batch_first=True)\n",
    "    batch_size, max_len = t.shape\n",
    "    t = torch.concat([t, torch.zeros((batch_size, 1))], -1)\n",
    "    t[torch.arange(batch_size), lens] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "832 μs ± 31.5 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit f1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3, 8, 8],\n",
       "        [0, 3, 6, 8]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create example data\n",
    "lens = [5, 8]\n",
    "x = [torch.randint(1, 10, (lens[0],)), torch.randint(1, 10, (lens[1],))]\n",
    "t = [torch.arange(6, 6 + lens[0]), torch.arange(6, 6 + lens[1])]\n",
    "\n",
    "\n",
    "class TULVAE(nn.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_hidden,\n",
    "        embedding_dim_loc,\n",
    "        embedding_dim_time,\n",
    "        latent_dim,\n",
    "        n_locs,\n",
    "        n_times,\n",
    "        n_users,\n",
    "        dropout,\n",
    "        n_layers,\n",
    "        timesteps_split,\n",
    "        device,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_hidden = n_hidden\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_locs = n_locs\n",
    "        self.n_times = n_times\n",
    "        self.n_layers = n_layers\n",
    "        self.n_users = n_users\n",
    "        self.embedding_dim_loc = embedding_dim_loc\n",
    "        self.embedding_dim_time = embedding_dim_time\n",
    "        self.timesteps_split = timesteps_split\n",
    "        self.embedding_dim = embedding_dim_time + embedding_dim_loc\n",
    "        self.sos_token = torch.tensor(n_locs + 1)\n",
    "        self.eos_token = torch.tensor(n_locs + 2)\n",
    "\n",
    "        self.short_encoder = nn.LSTM(\n",
    "            self.embedding_dim,\n",
    "            n_hidden,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.long_encoder = nn.LSTM(\n",
    "            self.embedding_dim,\n",
    "            n_hidden,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.classifier = nn.LSTM(\n",
    "            self.embedding_dim,\n",
    "            n_hidden,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.fc_clf = nn.Linear(2 * n_hidden, n_users)\n",
    "        self.fc_mean = nn.Linear(2 * n_hidden, n_users)\n",
    "        self.fc_var = nn.Linear(2 * n_hidden, n_users)\n",
    "        self.fc_decoder_in = nn.Linear() # softplus unit receiving hidden state of encoder\n",
    "        self.fc_decoder_out = nn.Linear() # for projecting the hidden state to number of classes\n",
    "\n",
    "        self.loc_embed = nn.Embedding(n_locs + 3, embedding_dim_loc, padding_idx=0)\n",
    "        self.time_embed = nn.Embedding(n_times + 1, embedding_dim_time, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, t, u):\n",
    "        input_long, short_input_decoder = self.short_forward(x, t)\n",
    "        long_input_decoder = self.long_forward(input_long)\n",
    "        mean, var = self.fc_mean()\n",
    "\n",
    "\n",
    "    def decode(self, h_enc, x):\n",
    "        \n",
    "        \n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def long_forward(self, input_packed):\n",
    "        out_packed, (h, c) = self.long_encoder(input_packed)\n",
    "        out, lens_long = pad_packed_sequence(out_packed)\n",
    "        return out.gather(1, lens_long)\n",
    "\n",
    "    def short_forward(self, x, t):\n",
    "\n",
    "        lens_short = torch.tensor([len(xi) for xi in x])\n",
    "        x_pad = pad_sequence(x, batch_first=True)\n",
    "        t_pad = pad_sequence(t, batch_first=True)\n",
    "\n",
    "        t_embed = self.time_embed(t_pad)\n",
    "        x_embed = self.loc_embed(x_pad)\n",
    "        # TODO: adapt encoder to only ingest x\n",
    "        xt_embed = torch.cat([x_embed, t_embed], dim=-1)\n",
    "\n",
    "        # Encode trajectory\n",
    "        x_packed = pack_padded_sequence(\n",
    "            xt_embed, lens_short, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        out_packed, (h, c) = self.short_encoder(x_packed)\n",
    "        out, lens_unpacked = pad_packed_sequence(out_packed)\n",
    "        # out.shape = (batch_size, max_len, 2*n_hidden)\n",
    "        idcs_long, lens_long = self.get_idcs_long(t_pad)\n",
    "        input_long_pad = out.gather(1, idcs_long)\n",
    "        input_long_packed = pack_padded_sequence(\n",
    "            input_long_pad, lens_long, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        input_decoder = out.gather(1, lens_unpacked)\n",
    "        return input_long_packed, input_decoder\n",
    "\n",
    "    def get_idcs_long(self, t_pad):\n",
    "        delta_t = t_pad - t_pad[:, 0, None]\n",
    "        idcs = torch.searchsorted(\n",
    "            delta_t, torch.arange(0, 24, 3)[None, :].expand(2, -1)\n",
    "        )\n",
    "        _, max_len = t_pad.shape\n",
    "        # Get place to cut off idcs\n",
    "        lens = (idcs < max_len).sum(-1) + 1\n",
    "        return idcs[:, : lens.max()], lens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lbn-trajectory-forecasting-OLKZBoZP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
